{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5d74013-04dd-470e-acb2-1bbcf6e97808",
   "metadata": {},
   "source": [
    "# new approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86fc8b65-8ab4-458b-b432-f7bc7418d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acc57051-5ead-4ab1-a432-d65ba9870085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be45f27-e21a-4c31-b29f-14f09ced5ce6",
   "metadata": {},
   "source": [
    "## 1. Initialize or Load Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf04b96-e9bb-4593-87ef-8a79a854cb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_categories_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Load categories and subcategories from a JSON file.\n",
    "    If the file does not exist or contains invalid JSON, initialize with default categories.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return initialize_empty_file(file_path)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        try:\n",
    "            return json.load(file)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Invalid JSON detected in {file_path}. Reinitializing the file.\")\n",
    "            return initialize_empty_file(file_path)\n",
    "\n",
    "\n",
    "def initialize_empty_file(file_path):\n",
    "    \"\"\"\n",
    "    Initialize a JSON file with a default dictionary and save it.\n",
    "    \"\"\"\n",
    "    categories = categories = {\n",
    "      \"Housing and Living Arrangements\": [\n",
    "        \"Rising Rent and Housing Costs\",\n",
    "        \"Finding Affordable and Safe Accommodation\",\n",
    "        \"Shared Housing and Roommate Dynamics\",\n",
    "        \"Adjusting to Different Housing Standards\",\n",
    "        \"Living with Extended Family or Parents\"\n",
    "      ],\n",
    "      \"Employment and Economic Opportunities\": [\n",
    "        \"Job Market Competition\",\n",
    "        \"Skill and Credential Recognition\",\n",
    "        \"Overqualification and Career Downgrades\",\n",
    "        \"Workplace Culture and Integration\",\n",
    "        \"Access to Professional Networking Opportunities\",\n",
    "        \"Balancing Work with Family Responsibilities\"\n",
    "      ],\n",
    "      \"Healthcare and Well-Being\": [\n",
    "        \"Understanding Local Healthcare Systems\",\n",
    "        \"Access to Primary and Emergency Care\",\n",
    "        \"Mental Health Resources Availability\",\n",
    "        \"Financial Barriers to Healthcare\",\n",
    "        \"Navigating Health Insurance Systems\",\n",
    "        \"Communicating Effectively with Healthcare Providers\"\n",
    "      ],\n",
    "      \"Social and Cultural Adjustment\": [\n",
    "        \"Building Community and Social Connections\",\n",
    "        \"Adapting to New Social Norms and Etiquette\",\n",
    "        \"Overcoming General Language Barriers in Social Contexts\",\n",
    "        \"Facing and Addressing Discrimination\",\n",
    "        \"Parenting Challenges in a New Cultural Environment\",\n",
    "        \"Celebrating and Preserving Cultural Traditions\"\n",
    "      ],\n",
    "      \"Legal and Bureaucratic Challenges\": [\n",
    "        \"Navigating Immigration and Residency Requirements\",\n",
    "        \"Understanding Tax Obligations\",\n",
    "        \"Securing Visas and Work Permits\",\n",
    "        \"Accessing Legal Aid or Advocacy Services\",\n",
    "        \"Filing Necessary Documentation for Families\",\n",
    "        \"Understanding Local Laws and Regulations\"\n",
    "      ],\n",
    "      \"Education and Personal Development\": [\n",
    "        \"Accessing Education for Children and Adults\",\n",
    "        \"Recognition of Previous Educational Credentials\",\n",
    "        \"Enrolling in Language and Integration Programs\",\n",
    "        \"Financial Barriers to Education and Training\",\n",
    "        \"Exploring New Career or Academic Opportunities\"\n",
    "      ],\n",
    "      \"Transportation and Mobility\": [\n",
    "        \"Navigating Public Transportation Systems\",\n",
    "        \"Obtaining Driver’s Licenses or Vehicle Registration\",\n",
    "        \"Cost and Accessibility of Transportation\",\n",
    "        \"Challenges in Rural or Suburban Mobility\",\n",
    "        \"Adjusting to New Traffic Rules and Regulations\",\n",
    "        \"Learning to Drive in a New Environment\"\n",
    "      ],\n",
    "      \"Financial and Budgeting Challenges\": [\n",
    "        \"Setting Up Bank Accounts and Building Credit\",\n",
    "        \"Understanding Local Taxes and Financial Systems\",\n",
    "        \"Managing Cost of Living in High-Expense Areas\",\n",
    "        \"Sending Money Abroad to Family\",\n",
    "        \"Planning and Budgeting for Financial Security\"\n",
    "      ],\n",
    "      \"Family Dynamics and Support\": [\n",
    "        \"Adjusting to Changing Family Roles\",\n",
    "        \"Reuniting with Family Across Borders\",\n",
    "        \"Supporting Children’s Educational and Social Needs\",\n",
    "        \"Caring for Aging Parents Remotely\",\n",
    "        \"Managing Relationships in Cross-Cultural Marriages\",\n",
    "        \"Strengthening Family Bonds in a New Environment\"\n",
    "      ],\n",
    "      \"Identity and Emotional Well-Being\": [\n",
    "        \"Coping with Culture Shock and Loneliness\",\n",
    "        \"Balancing Old and New Cultural Identities\",\n",
    "        \"Addressing Feelings of Isolation or Marginalization\",\n",
    "        \"Finding Support Networks for Emotional Health\",\n",
    "        \"Building a Sense of Belonging in the New Country\",\n",
    "        \"Overcoming Trauma and Resilience Building\"\n",
    "      ],\n",
    "      \"Public Safety and Security Concerns\": [\n",
    "        \"Perception of Neighbourhood Safety\",\n",
    "        \"Navigating Local Law Enforcement and Emergency Services\",\n",
    "        \"Personal Safety Strategies\",\n",
    "        \"Anxiety About Personal Safety\"\n",
    "      ],\n",
    "      \"Political Environment and Governance\": [\n",
    "        \"Understanding Local Political Parties and Policies\",\n",
    "        \"Concerns about Authoritarian or Repressive Tendencies\",\n",
    "        \"International Relations and Policy Impact on Immigrants\"\n",
    "      ],\n",
    "      \"Economic and Financial Stability\": [\n",
    "        \"Adapting to Market Fluctuations and Economic Changes\",\n",
    "        \"Building Resilience Against Economic Uncertainty\"\n",
    "      ]\n",
    "    }\n",
    "    save_categories_to_file(categories, file_path)\n",
    "    print(f\"Created a new categories file at: {file_path}\")\n",
    "    return categories\n",
    "\n",
    "\n",
    "def save_categories_to_file(categories, file_path):\n",
    "    \"\"\"\n",
    "    Save the updated categories and subcategories to a JSON file only if changes are detected.\n",
    "    \"\"\"\n",
    "    # Load existing categories from the file\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            try:\n",
    "                existing_categories = json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                existing_categories = {}\n",
    "    else:\n",
    "        existing_categories = {}\n",
    "\n",
    "    # Check if the categories have changed\n",
    "    if categories != existing_categories:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(categories, file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Updated categories saved to {file_path}\")\n",
    "    else:\n",
    "        print(\"No changes detected in categories. Skipping save.\")\n",
    "\n",
    "def parse_consolidation_response(response):\n",
    "    \"\"\"\n",
    "    Parse the GPT response to extract consolidated categories and subcategories.\n",
    "\n",
    "    Parameters:\n",
    "        response (str): The GPT response containing the updated categories and subcategories.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of consolidated categories and subcategories.\n",
    "    \"\"\"\n",
    "    consolidated_categories = {}\n",
    "\n",
    "    # Split the response into lines\n",
    "    lines = response.split(\"\\n\")\n",
    "    \n",
    "    current_category = None\n",
    "    for line in lines:\n",
    "        line = line.strip()  # Remove extra whitespace\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "\n",
    "        # Check for a category line (e.g., \"Housing and Rent:\")\n",
    "        if line.endswith(\":\"):\n",
    "            current_category = line[:-1].strip()  # Remove the trailing \":\"\n",
    "            consolidated_categories[current_category] = []\n",
    "        elif current_category:\n",
    "            # Treat non-category lines as subcategories\n",
    "            # Example format: \"    • Subcategory Name\"\n",
    "            if line.startswith(\"•\"):\n",
    "                subcategory = line[1:].strip()  # Remove the bullet point\n",
    "                consolidated_categories[current_category].append(subcategory)\n",
    "\n",
    "    return consolidated_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d0d743-b743-40ef-9197-25c2f6408e3c",
   "metadata": {},
   "source": [
    "## 2. Build the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ded97209-b25e-4697-a813-c6803722e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(categories, messages):\n",
    "    \"\"\"\n",
    "    Build a full prompt including current categories and subcategories\n",
    "    and the batch of messages to process.\n",
    "    \"\"\"\n",
    "    static_prompt = (\n",
    "        \"You are categorizing chat messages into predefined categories and \"\n",
    "        \"subcategories about specific, explicitly stated living problems or challenges \"\n",
    "        \"faced by Hong Kong people living in England.\\n\\n\"\n",
    "        \"Here are the current categories and subcategories:\\n\"\n",
    "    )\n",
    "\n",
    "    # Add categories and subcategories\n",
    "    for category, subs in categories.items():\n",
    "        static_prompt += f\"{category}\\n\"\n",
    "        for sub in subs:\n",
    "            static_prompt += f\"    • {sub}\\n\"\n",
    "\n",
    "    # Add the messages to categorize\n",
    "    static_prompt += \"\\nCurrent Batch of Messages:\\n\"\n",
    "    for i, message in enumerate(messages, start=1):\n",
    "        static_prompt += f\"{i}. {message}\\n\"\n",
    "\n",
    "    # Revised instructions\n",
    "    static_prompt += \"\"\"\n",
    "    Important Instructions:\n",
    "    1. Only categorize a message if it explicitly states a difficulty, hardship, or challenge related to living in England as a Hong Kong person.\n",
    "    \n",
    "       - For example: \"I cannot afford...\", \"I am struggling to...\", \"I have difficulty...\", \"I face a barrier...\", or any clear complaint about a problem.\n",
    "       - Just mentioning a topic (like a driving test, theory test, housing, or healthcare) is NOT enough. Must explicitly describe a problem.\n",
    "    \n",
    "    2. If the message:\n",
    "       - merely shares an event or promotional activity without mentioning any difficulty,\n",
    "       - mentions political activity, theory tests, driving tests, education, or any other topic without explicitly stating a personal difficulty,\n",
    "       - consists of general discussion or random content without highlighting a personal challenge,\n",
    "       - is focused on information sharing, or\n",
    "       - answers other people’s questions without expressing its own struggles,\n",
    "       \n",
    "       then it MUST be categorized as \"Uncategorized.\"\n",
    "    \n",
    "    3. DO NOT GUESS a problem. If not clear, choose \"Uncategorized.\"\n",
    "    \n",
    "    4. If the message describes a specific difficulty, hardship, or challenge for living in England that is not covered by any of the existing main categories or their subcategories, you must introduce a new category or subcategory:\n",
    "    \n",
    "       - If the difficulty logically fits within an existing main category but none of its current subcategories capture this new aspect, add a new subcategory under that existing main category. This new subcategory should clearly describe the specific difficulty mentioned in the message.\n",
    "       \n",
    "       - If the difficulty does not fit under any existing main category at all, create a completely new main category and a relevant first subcategory. Both the main category and subcategory names must clearly reflect the nature of the newly mentioned difficulty.\n",
    "    \n",
    "    Important:\n",
    "    - Do not reuse, copy, or refer to any categories or subcategories given as examples in these instructions. They are placeholders only.  \n",
    "    - Each time you create a new category or subcategory, invent a unique and contextually appropriate name that matches the difficulty described in the message.  \n",
    "    - The new category and/or subcategory must be directly related to the difficulty stated. If the message talks about a type of difficulty not previously covered, think of a descriptive name that conveys that exact challenge.\n",
    "    - After introducing a new category or subcategory, do not continue categorizing further messages in this batch. Stop immediately and return only the newly created category and subcategory.\n",
    "    \n",
    "    For clarity:\n",
    "    - If a message states a difficulty and it clearly doesn't match any existing categories or subcategories, you must come up with a new main category name and a new subcategory name that accurately describe this difficulty.  \n",
    "    - If a message states a difficulty that fits an existing category but needs a more specific angle not listed, add a new subcategory to that existing main category that directly addresses the difficulty mentioned.\n",
    "    \n",
    "    Do not guess or approximate. If the difficulty is new, create the category or subcategory right away. Do not return 'Uncategorized' when a difficulty is explicitly described and not covered by existing categories. Instead, produce a new category and/or subcategory as required.\n",
    "    5. No commentary or extra text outside the specified format.\n",
    "    \n",
    "    **Format:**\n",
    "    <message_number>. <Main Category>\n",
    "        • <Subcategory>\n",
    "    \n",
    "    OR if unrelated/unclassifiable:\n",
    "    <message_number>. Uncategorized\n",
    "    \n",
    "    **EXAMPLES:**\n",
    "    \n",
    "    - Unrelated event (no stated difficulty):\n",
    "      Message: \"CLS Hair Studio offering free haircuts on November 12.\"\n",
    "      Response:\n",
    "      1. Uncategorized\n",
    "    \n",
    "    - Political or asylum mention without stated difficulty:\n",
    "      Message: \"Hong Kong person got asylum in 2 years.\"\n",
    "      Response:\n",
    "      1. Uncategorized\n",
    "    \n",
    "    - Mentioning theory/driving test without difficulty:\n",
    "      Message: \"I took a theory test yesterday.\"\n",
    "      Response:\n",
    "      1. Uncategorized\n",
    "    \n",
    "    - Relevant (explicitly stating a difficulty):\n",
    "      Message: \"I am struggling to find affordable housing as a Hong Kong migrant in England.\"\n",
    "      Response:\n",
    "      1. Housing and Living Arrangements\n",
    "          • Finding Affordable and Safe Accommodation\n",
    "    \n",
    "    If the message does not explicitly say it is having trouble, difficulty, or a challenge, always choose 'Uncategorized'.\n",
    "    No guessing.\n",
    "    If a new category is needed, introduce it and stop.\n",
    "    \"\"\"\n",
    "\n",
    "    return static_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d4ca4-8ccc-4fc5-82f0-5e8ebba96b12",
   "metadata": {},
   "source": [
    "## 3. Process a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b02c15-db52-4748-aa0e-062446ad6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(client, prompt):\n",
    "    \"\"\"\n",
    "    Process a batch of messages using GPT.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {e}\")\n",
    "        raise  # Re-raise the exception to allow proper handling in the calling function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ca5efd-c58a-407b-a44e-924fbead64fb",
   "metadata": {},
   "source": [
    "## 4. Update Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "045ef898-db6a-4ecd-8971-666ad37233a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_categories_from_response(response, categories):\n",
    "    \"\"\"\n",
    "    Update categories and subcategories based on GPT's response.\n",
    "    Return True if a new main category was introduced, False otherwise.\n",
    "    \"\"\"\n",
    "    lines = response.split(\"\\n\")\n",
    "    new_category_introduced = False\n",
    "    current_main_category = None\n",
    "\n",
    "    for line in lines:\n",
    "        stripped_line = line.strip()\n",
    "        if stripped_line and stripped_line[0].isdigit() and \".\" in stripped_line:\n",
    "            # Category line format: \"1. Main Category\"\n",
    "            parts = stripped_line.split(\".\", 1)\n",
    "            cat_text = parts[1].strip()\n",
    "            if cat_text.lower() != \"uncategorized\":\n",
    "                current_main_category = cat_text\n",
    "            else:\n",
    "                current_main_category = None\n",
    "\n",
    "        elif stripped_line.startswith((\"•\", \"-\")) and current_main_category:\n",
    "            subcategory = stripped_line.lstrip(\"•-\").strip()\n",
    "            if subcategory.lower() == \"uncategorized\":\n",
    "                continue\n",
    "\n",
    "            # If current_main_category is new\n",
    "            if current_main_category not in categories:\n",
    "                categories[current_main_category] = []\n",
    "                new_category_introduced = True\n",
    "\n",
    "            if subcategory not in categories[current_main_category]:\n",
    "                categories[current_main_category].append(subcategory)\n",
    "\n",
    "    return new_category_introduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6998989b-086e-42c0-927f-e6da091a453d",
   "metadata": {},
   "source": [
    "## 5. Messages Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "043f6cc6-18cf-49c0-9810-e8e231103ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_messages_in_batches(client, df, categories_file, batch_size=50, overlap=20):\n",
    "#     categories = load_categories_from_file(categories_file)\n",
    "\n",
    "#     results = [(\"Uncategorized\", \"Uncategorized\")] * len(df)\n",
    "\n",
    "#     current_index = 0\n",
    "#     while current_index < len(df):\n",
    "#         end = min(current_index + batch_size, len(df))\n",
    "#         batch_messages = df[\"Chat Content\"].iloc[current_index:end].tolist()\n",
    "#         batch_messages = [str(msg) if isinstance(msg, str) else \"\" for msg in batch_messages]\n",
    "\n",
    "#         prompt = build_prompt(categories, batch_messages)\n",
    "#         response = process_batch(client, prompt)\n",
    "#         print(\"Raw GPT Response:\\n\", response)  # Print raw response for debugging\n",
    "\n",
    "#         if response == \"Error\":\n",
    "#             print(f\"Error processing batch {current_index} to {end}\")\n",
    "#             current_index = end\n",
    "#             continue\n",
    "\n",
    "#         # Check if new category introduced\n",
    "#         new_category_added = update_categories_from_response(response, categories)\n",
    "#         if new_category_added:\n",
    "#             save_categories_to_file(categories, categories_file)\n",
    "#             print(\"New category found and added. Restarting from the same batch with updated categories.\")\n",
    "\n",
    "#             # Clear results for this batch since we are reprocessing\n",
    "#             for i in range(current_index, end):\n",
    "#                 results[i] = (\"Uncategorized\", \"Uncategorized\")\n",
    "\n",
    "#             # Re-run same batch with updated categories\n",
    "#             continue\n",
    "\n",
    "#         # Parse the response lines into results\n",
    "#         response_lines = response.split(\"\\n\")\n",
    "#         batch_index = current_index\n",
    "#         expecting_subcategory = False\n",
    "#         current_category = None\n",
    "\n",
    "#         for line in response_lines:\n",
    "#             line_stripped = line.strip()\n",
    "#             if not line_stripped:\n",
    "#                 continue\n",
    "\n",
    "#             if line_stripped[0].isdigit() and \".\" in line_stripped:\n",
    "#                 parts = line_stripped.split(\".\", 1)\n",
    "#                 cat_text = parts[1].strip()\n",
    "#                 if cat_text.lower() == \"uncategorized\":\n",
    "#                     if batch_index < len(results):\n",
    "#                         results[batch_index] = (\"Uncategorized\", \"Uncategorized\")\n",
    "#                         batch_index += 1\n",
    "#                     expecting_subcategory = False\n",
    "#                 else:\n",
    "#                     current_category = cat_text\n",
    "#                     expecting_subcategory = True\n",
    "#                 continue\n",
    "\n",
    "#             if expecting_subcategory and line_stripped.startswith((\"•\", \"-\")):\n",
    "#                 clean_line = line_stripped.lstrip(\"•-\").strip()\n",
    "#                 subcategory = clean_line\n",
    "#                 if batch_index < len(results):\n",
    "#                     results[batch_index] = (current_category, subcategory)\n",
    "#                     batch_index += 1\n",
    "#                 expecting_subcategory = False\n",
    "#                 continue\n",
    "\n",
    "#             # If format not followed, uncategorized\n",
    "#             if batch_index < len(results):\n",
    "#                 results[batch_index] = (\"Uncategorized\", \"Uncategorized\")\n",
    "#                 batch_index += 1\n",
    "#             expecting_subcategory = False\n",
    "\n",
    "#         current_index = end\n",
    "\n",
    "#     df[\"Category\"] = [r[0] for r in results]\n",
    "#     df[\"Subcategory\"] = [r[1] for r in results]\n",
    "\n",
    "#     print(\"Final DataFrame with Categories and Subcategories:\\n\", df[[\"Category\", \"Subcategory\"]].head(10))\n",
    "#     # df.to_csv(\"./categorized_messages.csv\", index=False)\n",
    "#     # print(\"Updated CSV saved to ./categorized_messages.csv\")\n",
    "\n",
    "#     return df, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9c057d5-21a9-4177-9f6d-74fc736650cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_messages_in_batches(client, df, categories_file, token_limit=3000, overlap_token_limit=500):\n",
    "#     \"\"\"\n",
    "#     Processes messages in batches based on token counts with dynamic overlap based on tokens.\n",
    "\n",
    "#     Args:\n",
    "#         client: The client to process batches (e.g., GPT-4 API client).\n",
    "#         df (pd.DataFrame): DataFrame containing messages with 'Chat Content' and 'Token Count' columns.\n",
    "#         categories_file (str): Path to the categories file.\n",
    "#         token_limit (int, optional): Maximum number of tokens per batch. Defaults to 3000.\n",
    "#         overlap_token_limit (int, optional): Maximum number of tokens for overlap between batches. Defaults to 500.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: Updated DataFrame with 'Category' and 'Subcategory' columns.\n",
    "#         list: Updated list of categories.\n",
    "#     \"\"\"\n",
    "#     categories = load_categories_from_file(categories_file)\n",
    "\n",
    "#     # Initialize results with default categories\n",
    "#     results = [(\"Uncategorized\", \"Uncategorized\")] * len(df)\n",
    "\n",
    "#     current_index = 0\n",
    "\n",
    "#     while current_index < len(df):\n",
    "#         batch_start = current_index\n",
    "\n",
    "#         # Calculate overlap based on token counts\n",
    "#         overlap_start = max(0, current_index - 1)  # Start from the previous row\n",
    "#         overlap_tokens = 0\n",
    "#         overlap_indices = []\n",
    "\n",
    "#         # Accumulate overlap rows without exceeding overlap_token_limit\n",
    "#         while overlap_start >= 0 and overlap_tokens + df.iloc[overlap_start]['Token Count'] <= overlap_token_limit:\n",
    "#             overlap_tokens += df.iloc[overlap_start]['Token Count']\n",
    "#             overlap_indices.insert(0, overlap_start)  # Insert at the beginning to maintain order\n",
    "#             overlap_start -= 1\n",
    "\n",
    "#         # Define the new batch start including overlap\n",
    "#         effective_batch_start = overlap_indices[0] if overlap_indices else batch_start\n",
    "\n",
    "#         # Initialize batch_end to start accumulating tokens\n",
    "#         batch_end = batch_start\n",
    "#         total_tokens = 0\n",
    "\n",
    "#         # Accumulate rows until token_limit is reached\n",
    "#         while batch_end < len(df) and total_tokens + df.iloc[batch_end]['Token Count'] <= token_limit:\n",
    "#             total_tokens += df.iloc[batch_end]['Token Count']\n",
    "#             batch_end += 1\n",
    "\n",
    "#         # If no rows were added (single row exceeds token_limit), include it anyway\n",
    "#         if batch_start == batch_end:\n",
    "#             batch_end += 1\n",
    "\n",
    "#         # Include overlap in the batch\n",
    "#         final_batch_start = effective_batch_start\n",
    "#         final_batch_end = batch_end\n",
    "\n",
    "#         # Extract batch messages with overlap\n",
    "#         batch_messages = df[\"Chat Content\"].iloc[final_batch_start:final_batch_end].tolist()\n",
    "#         batch_messages = [str(msg) if isinstance(msg, str) else \"\" for msg in batch_messages]\n",
    "\n",
    "#         # Build prompt and process batch\n",
    "#         prompt = build_prompt(categories, batch_messages)\n",
    "#         response = process_batch(client, prompt)\n",
    "#         print(f\"Processing rows {final_batch_start} to {final_batch_end} with total tokens {total_tokens} and overlap tokens {overlap_tokens}\")\n",
    "#         print(\"Raw GPT Response:\\n\", response)  # Print raw response for debugging\n",
    "\n",
    "#         if response == \"Error\":\n",
    "#             print(f\"Error processing batch {final_batch_start} to {final_batch_end}\")\n",
    "#             # Skip to the next batch, considering overlap\n",
    "#             current_index = final_batch_end - 1  # Move back by one row for potential overlap\n",
    "#             if current_index < 0:\n",
    "#                 current_index = 0\n",
    "#             continue\n",
    "\n",
    "#         # Check if new category introduced\n",
    "#         new_category_added = update_categories_from_response(response, categories)\n",
    "#         if new_category_added:\n",
    "#             save_categories_to_file(categories, categories_file)\n",
    "#             print(\"New category found and added. Restarting from the same batch with updated categories.\")\n",
    "\n",
    "#             # Clear results for this batch since we are reprocessing\n",
    "#             for i in range(final_batch_start, final_batch_end):\n",
    "#                 results[i] = (\"Uncategorized\", \"Uncategorized\")\n",
    "\n",
    "#             # Re-run same batch with updated categories\n",
    "#             continue\n",
    "\n",
    "#         # Parse the response lines into results\n",
    "#         response_lines = response.split(\"\\n\")\n",
    "#         batch_index = final_batch_start\n",
    "#         expecting_subcategory = False\n",
    "#         current_category = None\n",
    "\n",
    "#         for line in response_lines:\n",
    "#             line_stripped = line.strip()\n",
    "#             if not line_stripped:\n",
    "#                 continue\n",
    "\n",
    "#             if line_stripped[0].isdigit() and \".\" in line_stripped:\n",
    "#                 parts = line_stripped.split(\".\", 1)\n",
    "#                 cat_text = parts[1].strip()\n",
    "#                 if cat_text.lower() == \"uncategorized\":\n",
    "#                     if batch_index < len(results):\n",
    "#                         results[batch_index] = (\"Uncategorized\", \"Uncategorized\")\n",
    "#                         batch_index += 1\n",
    "#                     expecting_subcategory = False\n",
    "#                 else:\n",
    "#                     current_category = cat_text\n",
    "#                     expecting_subcategory = True\n",
    "#                 continue\n",
    "\n",
    "#             if expecting_subcategory and line_stripped.startswith((\"•\", \"-\")):\n",
    "#                 clean_line = line_stripped.lstrip(\"•-\").strip()\n",
    "#                 subcategory = clean_line\n",
    "#                 if batch_index < len(results):\n",
    "#                     results[batch_index] = (current_category, subcategory)\n",
    "#                     batch_index += 1\n",
    "#                 expecting_subcategory = False\n",
    "#                 continue\n",
    "\n",
    "#             # If format not followed, uncategorized\n",
    "#             if batch_index < len(results):\n",
    "#                 results[batch_index] = (\"Uncategorized\", \"Uncategorized\")\n",
    "#                 batch_index += 1\n",
    "#             expecting_subcategory = False\n",
    "\n",
    "#         # Move to the next batch\n",
    "#         current_index = final_batch_end\n",
    "\n",
    "#     # Assign the results to the DataFrame\n",
    "#     df[\"Category\"] = [r[0] for r in results]\n",
    "#     df[\"Subcategory\"] = [r[1] for r in results]\n",
    "\n",
    "#     print(\"Final DataFrame with Categories and Subcategories:\\n\", df[[\"Category\", \"Subcategory\"]].head(10))\n",
    "#     # Optionally save the updated DataFrame\n",
    "#     # df.to_csv(\"./categorized_messages.csv\", index=False)\n",
    "#     # print(\"Updated CSV saved to ./categorized_messages.csv\")\n",
    "\n",
    "#     return df, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "315dd554-a670-4f42-a674-8cf34db43572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## okay but just label a batch wrong sometimes not start from 1 and it doenst know how to match it on excel\n",
    "# def process_messages_in_batches(client, df, categories_file, token_limit=3000, overlap_token_limit=500):\n",
    "#     \"\"\"\n",
    "#     Processes messages in batches based on token counts with dynamic overlap based on tokens.\n",
    "\n",
    "#     Args:\n",
    "#         client: The client to process batches (e.g., GPT-4 API client).\n",
    "#         df (pd.DataFrame): DataFrame containing messages with 'Chat Content' and 'Token Count' columns.\n",
    "#         categories_file (str): Path to the categories file.\n",
    "#         token_limit (int, optional): Maximum number of tokens per batch. Defaults to 3000.\n",
    "#         overlap_token_limit (int, optional): Maximum number of tokens for overlap between batches. Defaults to 500.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: Updated DataFrame with 'Category' and 'Subcategory' columns.\n",
    "#         list: Updated list of categories.\n",
    "#     \"\"\"\n",
    "#     categories = load_categories_from_file(categories_file)\n",
    "\n",
    "#     # Initialize results with default categories\n",
    "#     results = [(\"Uncategorized\", \"Uncategorized\")] * len(df)\n",
    "\n",
    "#     current_index = 0\n",
    "\n",
    "#     while current_index < len(df):\n",
    "#         batch_start = current_index\n",
    "\n",
    "#         # Calculate overlap based on token counts\n",
    "#         overlap_start = max(0, current_index - 1)  # Start from the previous row\n",
    "#         overlap_tokens = 0\n",
    "#         overlap_indices = []\n",
    "\n",
    "#         # Accumulate overlap rows without exceeding overlap_token_limit\n",
    "#         while overlap_start >= 0 and overlap_tokens + df.iloc[overlap_start]['Token Count'] <= overlap_token_limit:\n",
    "#             overlap_tokens += df.iloc[overlap_start]['Token Count']\n",
    "#             overlap_indices.insert(0, overlap_start)  # Insert at the beginning to maintain order\n",
    "#             overlap_start -= 1\n",
    "\n",
    "#         # Define the new batch start including overlap\n",
    "#         effective_batch_start = overlap_indices[0] if overlap_indices else batch_start\n",
    "\n",
    "#         # Initialize batch_end to start accumulating tokens\n",
    "#         batch_end = batch_start\n",
    "#         total_tokens = 0\n",
    "\n",
    "#         # Accumulate rows until token_limit is reached\n",
    "#         while batch_end < len(df) and total_tokens + df.iloc[batch_end]['Token Count'] <= token_limit:\n",
    "#             total_tokens += df.iloc[batch_end]['Token Count']\n",
    "#             batch_end += 1\n",
    "\n",
    "#         # If no rows were added (single row exceeds token_limit), include it anyway\n",
    "#         if batch_start == batch_end:\n",
    "#             batch_end += 1\n",
    "\n",
    "#         # Include overlap in the batch\n",
    "#         final_batch_start = effective_batch_start\n",
    "#         final_batch_end = batch_end\n",
    "\n",
    "#         # Extract batch messages with overlap\n",
    "#         batch_messages = df[\"Chat Content\"].iloc[final_batch_start:final_batch_end].tolist()\n",
    "#         batch_messages = [str(msg) if isinstance(msg, str) else \"\" for msg in batch_messages]\n",
    "\n",
    "#         # Build prompt and process batch\n",
    "#         prompt = build_prompt(categories, batch_messages)\n",
    "#         response = process_batch(client, prompt)\n",
    "#         print(f\"Processing rows {final_batch_start} to {final_batch_end} with total tokens {total_tokens} and overlap tokens {overlap_tokens}\")\n",
    "#         print(\"Raw GPT Response:\\n\", response)  # Print raw response for debugging\n",
    "\n",
    "#         if response == \"Error\":\n",
    "#             print(f\"Error processing batch {final_batch_start} to {final_batch_end}\")\n",
    "#             # Skip to the next batch, considering overlap\n",
    "#             current_index = final_batch_end - 1  # Move back by one row for potential overlap\n",
    "#             if current_index < 0:\n",
    "#                 current_index = 0\n",
    "#             continue\n",
    "\n",
    "#         # Check if new category introduced\n",
    "#         new_category_added = update_categories_from_response(response, categories)\n",
    "#         if new_category_added:\n",
    "#             save_categories_to_file(categories, categories_file)\n",
    "#             print(\"New category found and added. Restarting from the same batch with updated categories.\")\n",
    "\n",
    "#             # Clear results for this batch since we are reprocessing\n",
    "#             for i in range(final_batch_start, final_batch_end):\n",
    "#                 results[i] = (\"Uncategorized\", \"Uncategorized\")\n",
    "\n",
    "#             # Re-run same batch with updated categories\n",
    "#             continue\n",
    "\n",
    "#         # Parse the response lines into results\n",
    "#         response_lines = response.split(\"\\n\")\n",
    "#         batch_index = final_batch_start\n",
    "\n",
    "#         for line in response_lines:\n",
    "#             line_stripped = line.strip()\n",
    "#             if not line_stripped:\n",
    "#                 continue\n",
    "\n",
    "#             if line_stripped[0].isdigit() and \".\" in line_stripped:\n",
    "#                 parts = line_stripped.split(\".\", 1)\n",
    "#                 cat_text = parts[1].strip()\n",
    "#                 if cat_text.lower() == \"uncategorized\":\n",
    "#                     if batch_index < len(results):\n",
    "#                         # Only assign if it's a new row, not an overlap\n",
    "#                         if batch_index >= batch_start:\n",
    "#                             results[batch_index] = (\"Uncategorized\", \"Uncategorized\")\n",
    "#                     batch_index += 1\n",
    "#                     expecting_subcategory = False\n",
    "#                 else:\n",
    "#                     current_category = cat_text\n",
    "#                     expecting_subcategory = True\n",
    "#                 continue\n",
    "\n",
    "#             if expecting_subcategory and line_stripped.startswith((\"•\", \"-\")):\n",
    "#                 clean_line = line_stripped.lstrip(\"•-\").strip()\n",
    "#                 subcategory = clean_line\n",
    "#                 if batch_index < len(results):\n",
    "#                     # Only assign if it's a new row, not an overlap\n",
    "#                     if batch_index >= batch_start:\n",
    "#                         results[batch_index] = (current_category, subcategory)\n",
    "#                 batch_index += 1\n",
    "#                 expecting_subcategory = False\n",
    "#                 continue\n",
    "\n",
    "#             # If format not followed, uncategorized\n",
    "#             if batch_index < len(results):\n",
    "#                 if batch_index >= batch_start:\n",
    "#                     results[batch_index] = (\"Uncategorized\", \"Uncategorized\")\n",
    "#                 batch_index += 1\n",
    "#             expecting_subcategory = False\n",
    "\n",
    "#         # Move to the next batch\n",
    "#         current_index = final_batch_end\n",
    "\n",
    "#     # Assign the results to the DataFrame\n",
    "#     df[\"Category\"] = [r[0] for r in results]\n",
    "#     df[\"Subcategory\"] = [r[1] for r in results]\n",
    "\n",
    "#     print(\"Final DataFrame with Categories and Subcategories:\\n\", df[[\"Category\", \"Subcategory\"]].head(10))\n",
    "#     # Optionally save the updated DataFrame\n",
    "#     # df.to_csv(\"./categorized_messages.csv\", index=False)\n",
    "#     # print(\"Updated CSV saved to ./categorized_messages.csv\")\n",
    "\n",
    "#     return df, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "783fc9d1-a02f-4cab-a816-26b31b67830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_messages_in_batches(client, df, categories_file, token_limit=3000, overlap_token_limit=500):\n",
    "    \"\"\"\n",
    "    Processes messages in batches based on token counts with dynamic overlap based on tokens.\n",
    "\n",
    "    Args:\n",
    "        client: The client to process batches (e.g., GPT-4 API client).\n",
    "        df (pd.DataFrame): DataFrame containing messages with 'Chat Content' and 'Token Count' columns.\n",
    "        categories_file (str): Path to the categories file.\n",
    "        token_limit (int, optional): Maximum number of tokens per batch. Defaults to 3000.\n",
    "        overlap_token_limit (int, optional): Maximum number of tokens for overlap between batches. Defaults to 500.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with 'Category' and 'Subcategory' columns.\n",
    "        list: Updated list of categories.\n",
    "    \"\"\"\n",
    "    categories = load_categories_from_file(categories_file)\n",
    "\n",
    "    # Initialize results with default categories\n",
    "    results = [(\"Uncategorized\", \"Uncategorized\")] * len(df)\n",
    "\n",
    "    current_index = 0\n",
    "\n",
    "    while current_index < len(df):\n",
    "        batch_start = current_index\n",
    "\n",
    "        # Initialize overlap_indices and overlap_tokens\n",
    "        overlap_indices = []\n",
    "        overlap_tokens = 0\n",
    "\n",
    "        # Calculate overlap only if not the first batch\n",
    "        if current_index > 0:\n",
    "            overlap_start = current_index - 1  # Start from the previous row\n",
    "\n",
    "            # Accumulate overlap rows without exceeding overlap_token_limit\n",
    "            while overlap_start >= 0 and overlap_tokens + df.iloc[overlap_start]['Token Count'] <= overlap_token_limit:\n",
    "                overlap_tokens += df.iloc[overlap_start]['Token Count']\n",
    "                overlap_indices.insert(0, overlap_start)  # Insert at the beginning to maintain order\n",
    "                overlap_start -= 1\n",
    "\n",
    "        # Define the new batch start including overlap\n",
    "        effective_batch_start = overlap_indices[0] if overlap_indices else batch_start\n",
    "        overlap_count = len(overlap_indices)  # Number of overlapping rows\n",
    "\n",
    "        # Initialize batch_end to start accumulating tokens\n",
    "        batch_end = batch_start\n",
    "        total_tokens = 0\n",
    "\n",
    "        # Accumulate rows until token_limit is reached\n",
    "        while batch_end < len(df) and total_tokens + df.iloc[batch_end]['Token Count'] <= token_limit:\n",
    "            total_tokens += df.iloc[batch_end]['Token Count']\n",
    "            batch_end += 1\n",
    "\n",
    "        # If no rows were added (single row exceeds token_limit), include it anyway\n",
    "        if batch_start == batch_end:\n",
    "            batch_end += 1\n",
    "\n",
    "        # Include overlap in the batch\n",
    "        final_batch_start = effective_batch_start\n",
    "        final_batch_end = batch_end\n",
    "\n",
    "        # Extract batch messages with overlap\n",
    "        batch_messages = df[\"Chat Content\"].iloc[final_batch_start:final_batch_end].tolist()\n",
    "        batch_messages = [str(msg) if isinstance(msg, str) else \"\" for msg in batch_messages]\n",
    "\n",
    "        # Build prompt and process batch\n",
    "        prompt = build_prompt(categories, batch_messages)\n",
    "        response = process_batch(client, prompt)\n",
    "        print(f\"Processing rows {final_batch_start} to {final_batch_end} with total tokens {total_tokens} and overlap tokens {overlap_tokens}\")\n",
    "        print(\"Raw GPT Response:\\n\", response)  # Print raw response for debugging\n",
    "\n",
    "        if response == \"Error\":\n",
    "            print(f\"Error processing batch {final_batch_start} to {final_batch_end}\")\n",
    "            # Skip to the next batch, considering overlap\n",
    "            current_index = final_batch_end - overlap_count if overlap_count > 0 else final_batch_end\n",
    "            if current_index < 0:\n",
    "                current_index = 0\n",
    "            continue\n",
    "\n",
    "        # Check if new category introduced\n",
    "        new_category_added = update_categories_from_response(response, categories)\n",
    "        if new_category_added:\n",
    "            save_categories_to_file(categories, categories_file)\n",
    "            print(\"New category found and added. Restarting from the same batch with updated categories.\")\n",
    "\n",
    "            # Clear results for this batch since we are reprocessing\n",
    "            for i in range(final_batch_start, final_batch_end):\n",
    "                results[i] = (\"Uncategorized\", \"Uncategorized\")\n",
    "\n",
    "            # Re-run same batch with updated categories\n",
    "            continue\n",
    "\n",
    "        # Parse the response lines into labels\n",
    "        response_lines = response.split(\"\\n\")\n",
    "        labels = []\n",
    "        current_category = None\n",
    "\n",
    "        for line in response_lines:\n",
    "            line_stripped = line.strip()\n",
    "            if not line_stripped:\n",
    "                continue\n",
    "\n",
    "            # Detect category lines (e.g., \"1. Category Name\")\n",
    "            if \".\" in line_stripped:\n",
    "                parts = line_stripped.split(\".\", 1)\n",
    "                cat_text = parts[1].strip()\n",
    "                if cat_text.lower() == \"uncategorized\":\n",
    "                    labels.append((\"Uncategorized\", \"Uncategorized\"))\n",
    "                    current_category = None\n",
    "                else:\n",
    "                    current_category = cat_text\n",
    "            # Detect subcategory lines (e.g., \"• Subcategory Name\")\n",
    "            elif line_stripped.startswith((\"•\", \"-\")) and current_category:\n",
    "                clean_line = line_stripped.lstrip(\"•-\").strip()\n",
    "                labels.append((current_category, clean_line))\n",
    "            else:\n",
    "                # If format not followed, mark as Uncategorized\n",
    "                labels.append((\"Uncategorized\", \"Uncategorized\"))\n",
    "\n",
    "        # Determine the indices for new rows (excluding overlaps)\n",
    "        new_batch_start = batch_start\n",
    "        new_batch_end = final_batch_end\n",
    "        new_rows_count = new_batch_end - batch_start\n",
    "        new_labels = labels[overlap_count:]  # Skip labels corresponding to overlapping rows\n",
    "\n",
    "        if len(new_labels) != (new_batch_end - batch_start):\n",
    "            print(f\"Warning: Number of labels ({len(new_labels)}) does not match number of new rows ({new_batch_end - batch_start}).\")\n",
    "            # Optionally, handle this discrepancy as needed\n",
    "            # For now, fill the missing labels with \"Uncategorized\"\n",
    "            if len(new_labels) < (new_batch_end - batch_start):\n",
    "                new_labels += [(\"Uncategorized\", \"Uncategorized\")] * ((new_batch_end - batch_start) - len(new_labels))\n",
    "            elif len(new_labels) > (new_batch_end - batch_start):\n",
    "                new_labels = new_labels[:(new_batch_end - batch_start)]\n",
    "\n",
    "        # Assign labels to new rows only\n",
    "        for i, label in enumerate(new_labels):\n",
    "            row_index = batch_start + i\n",
    "            results[row_index] = label\n",
    "\n",
    "        # Move to the next batch\n",
    "        current_index = final_batch_end\n",
    "\n",
    "    # Assign the results to the DataFrame\n",
    "    df[\"Category\"] = [r[0] for r in results]\n",
    "    df[\"Subcategory\"] = [r[1] for r in results]\n",
    "\n",
    "    print(\"Final DataFrame with Categories and Subcategories:\\n\", df[[\"Category\", \"Subcategory\"]].head(10))\n",
    "    # Optionally save the updated DataFrame\n",
    "    # df.to_csv(\"./categorized_messages.csv\", index=False)\n",
    "    # print(\"Updated CSV saved to ./categorized_messages.csv\")\n",
    "\n",
    "    return df, categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8428c3-f05e-4a07-afad-5b065551e092",
   "metadata": {},
   "source": [
    "## 7. Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d30f4215-5d38-49c2-9c23-2f37b8270a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def preprocess_message(user, message):\n",
    "    \"\"\"\n",
    "    Format a single message with the username and normalize UTF-8 encoding.\n",
    "    Replaces existing colons in the message to avoid ambiguity.\n",
    "    \"\"\"\n",
    "    if not isinstance(message, str):\n",
    "        message = \"\"\n",
    "    \n",
    "    # Normalize message to UTF-8\n",
    "    message = message.encode(\"utf-8\").decode(\"utf-8\")\n",
    "    \n",
    "    # Replace existing colons to avoid ambiguity\n",
    "    message = message.replace(\":\", \" |\")\n",
    "    \n",
    "    # Format the message with the username\n",
    "    return f'{user}: {message}' if user else message\n",
    "\n",
    "def preprocess_messages_with_usernames(df):\n",
    "    \"\"\"\n",
    "    Preprocess messages by adding usernames and normalizing UTF-8 encoding.\n",
    "    Incorporates the original Filter 1 logic and adds selected stricter conditions\n",
    "    from Filter 2 (symbols_only, numeric_only, too_long, and suspicious links).\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the required columns exist\n",
    "    if \"Who\" not in df.columns or \"Chat Content\" not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'Who' and 'Chat Content' columns.\")\n",
    "\n",
    "    # Normalize 'Who' and 'Chat Content' to UTF-8\n",
    "    df['Who'] = df['Who'].apply(lambda x: x.encode(\"utf-8\").decode(\"utf-8\") if isinstance(x, str) else x)\n",
    "    df['Chat Content'] = df['Chat Content'].apply(lambda x: x.encode(\"utf-8\").decode(\"utf-8\") if isinstance(x, str) else \"\")\n",
    "\n",
    "    # Define the array of blocked phrases (original filter 1)\n",
    "    blocked_phrases = [\n",
    "        '首充入', '秒到帳', '每筆送', '獎金高達', '報名參加', '報名:', '報名：', '，報名', ', 報名', '優惠', '日期：', '時間：', '地點：', \n",
    "        '加入TG', '全文：', '報導', '當年今日', '現正招募', '專訪','拉群', '点我', '有意請', '立即申請：', '關注我們', '尋失物', 'LIHKG', \n",
    "        'lih.kg', 'play.google.com', 'Find out more', '得獎內容', '問卷連結', '公告：', 'Happy birthday', '生日快樂'\n",
    "    ]\n",
    "\n",
    "    # Define allowed domains for links (from filter 2)\n",
    "    allowed_domains = ['.uk', '.edu']\n",
    "\n",
    "    # Define conditions from original filter 1\n",
    "    cond_empty = df['Chat Content'].str.strip() == ''\n",
    "    cond_nan = df['Chat Content'].isna() | (df['Chat Content'].str.strip().str.upper() == 'NAN')\n",
    "    cond_link_only = df['Chat Content'].str.strip().str.match(r'^(https?://\\S+|www\\.\\S+)$', na=False)\n",
    "    cond_emoji_only = df['Chat Content'].str.match(r'^[\\U0001F300-\\U0001F6FF]+$', na=False)\n",
    "    cond_emoji_with_link = df['Chat Content'].str.match(r'^[\\U0001F300-\\U0001F6FF]+\\s+https?://\\S+$', na=False)\n",
    "    cond_who_contains_bot = df['Who'].str.contains('bot', case=False, na=False)\n",
    "    cond_blocked_phrases = df['Chat Content'].str.contains('|'.join(map(re.escape, blocked_phrases)), case=False, na=False)\n",
    "\n",
    "    # Calculate char count\n",
    "    df['CharCount'] = df['Chat Content'].str.len()\n",
    "\n",
    "    # Conditions from original filter 1\n",
    "    cond_two_hash_and_word_count = (\n",
    "        (df['Chat Content'].str.count('#') >= 2) & (df['CharCount'] > 80)\n",
    "    )\n",
    "\n",
    "    cond_instagram_and_facebook_words = (\n",
    "        df['Chat Content'].str.contains('instagram', case=False, na=False) & \n",
    "        df['Chat Content'].str.contains('facebook', case=False, na=False)\n",
    "    )\n",
    "\n",
    "    cond_instagram_and_facebook_links = (\n",
    "        df['Chat Content'].str.contains(r'instagram\\.com', case=False, na=False) & \n",
    "        df['Chat Content'].str.contains(r'facebook\\.com', case=False, na=False)\n",
    "    )\n",
    "\n",
    "    cond_instagram_and_facebook = cond_instagram_and_facebook_words | cond_instagram_and_facebook_links\n",
    "\n",
    "    cond_long_no_chinese = (df['CharCount'] > 700) & (~df['Chat Content'].str.contains(r'[\\u4e00-\\u9fff]', na=False))\n",
    "    cond_short_no_chinese = (df['CharCount'] == 1) & (~df['Chat Content'].str.contains(r'[\\u4e00-\\u9fff]', na=False))\n",
    "\n",
    "    # New conditions from filter 2 to include:\n",
    "    cond_symbols_only = df['Chat Content'].str.match(r'^[\\W_]+$', na=False)\n",
    "    cond_numeric_only = df['Chat Content'].str.match(r'^\\d+$', na=False)\n",
    "    cond_too_long = df['CharCount'] > 1000\n",
    "\n",
    "    # Suspicious links (no allowed domain), taken from filter 2 logic\n",
    "    cond_links_or_hashtags = (\n",
    "        df['Chat Content'].str.contains(r'(?:https?://|www\\.)', na=False) &\n",
    "        ~df['Chat Content'].str.contains('|'.join(map(re.escape, allowed_domains)), na=False)\n",
    "    )\n",
    "\n",
    "    # Remove rows where CharCount <= 5\n",
    "    cond_char_count_short = df['CharCount'] <= 5\n",
    "\n",
    "    # New condition: One or two English words with optional symbols or emojis\n",
    "    cond_one_two_words_with_emojis_or_symbols = df['Chat Content'].str.match(\n",
    "        r'^\\s*[\\W_]*[a-zA-Z]+(?:\\s+[a-zA-Z]+)?[\\W_]*\\s*$', na=False\n",
    "    )\n",
    "\n",
    "    # Combine all conditions using logical OR\n",
    "    # Update the combined mask\n",
    "    mask = (\n",
    "        cond_empty |\n",
    "        cond_nan |\n",
    "        cond_link_only |\n",
    "        cond_emoji_only |\n",
    "        cond_emoji_with_link |\n",
    "        cond_who_contains_bot |\n",
    "        cond_blocked_phrases |\n",
    "        cond_two_hash_and_word_count |\n",
    "        cond_instagram_and_facebook |\n",
    "        cond_long_no_chinese |\n",
    "        cond_short_no_chinese |\n",
    "        cond_symbols_only |\n",
    "        cond_numeric_only |\n",
    "        cond_too_long |\n",
    "        cond_links_or_hashtags |\n",
    "        cond_char_count_short |  # Add this condition\n",
    "        cond_one_two_words_with_emojis_or_symbols  # New condition\n",
    "    )\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    df = df[~mask].copy()\n",
    "\n",
    "    # Preprocess messages (create 'Processed Content')\n",
    "    df[\"Processed Content\"] = df.apply(\n",
    "        lambda row: preprocess_message(row[\"Who\"], row[\"Chat Content\"]),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to generate a random \"user\" ID\n",
    "def generate_user_id():\n",
    "    return f\"user{random.randint(10000, 99999)}\"\n",
    "\n",
    "# Function to assign random user IDs to empty rows in the 'Who' column\n",
    "def assign_user_ids(df, column_name):\n",
    "    current_user_id = None\n",
    "    for index in df.index:\n",
    "        if pd.isna(df.at[index, column_name]) or df.at[index, column_name] == \"\":  # Check if the column is empty\n",
    "            if current_user_id is None:\n",
    "                current_user_id = generate_user_id()\n",
    "            df.at[index, column_name] = current_user_id\n",
    "        else:\n",
    "            current_user_id = None  # Reset when encountering a non-empty row\n",
    "    return df\n",
    "\n",
    "# Main function to process the file\n",
    "def process_who_column(input_file):\n",
    "    # Load the CSV into a pandas DataFrame\n",
    "    df = pd.read_csv(input_file)\n",
    "    # Apply the function to assign user IDs\n",
    "    df = assign_user_ids(df, column_name='Who')\n",
    "    # Return the modified DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c25ac89-6738-4d45-a0a1-22438feb9836",
   "metadata": {},
   "source": [
    "## 8. Run the Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75ac33d6-2e92-42f0-a5c1-985212b6bd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import pandas as pd\n",
    "\n",
    "def count_tokens(text, encoding=\"cl100k_base\"):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a text using tiktoken.\n",
    "    :param text: The input text string.\n",
    "    :param encoding: The encoding model to use (default: \"cl100k_base\").\n",
    "    :return: The number of tokens in the input text.\n",
    "    \"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(encoding)\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def add_token_count_column(df, column_name=\"Processed Content\", new_column_name=\"Token Count\"):\n",
    "    \"\"\"\n",
    "    Add a column for token counts to a DataFrame.\n",
    "    :param df: The input DataFrame.\n",
    "    :param column_name: The name of the column containing text.\n",
    "    :param new_column_name: The name of the new column for token counts.\n",
    "    :return: The updated DataFrame with the new column.\n",
    "    \"\"\"\n",
    "    df[new_column_name] = df[column_name].apply(lambda x: count_tokens(str(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24d79c72-dd93-47f1-bbda-d79eeff384e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # File paths\n",
    "    categories_file = \"./categories.json\"\n",
    "    messages_file = \"./9000_messages.csv\"\n",
    "    output_file = \"./categorized_300_messages.csv\"\n",
    "\n",
    "    # Load messages\n",
    "    df = pd.read_csv(messages_file)\n",
    "\n",
    "    # Filling empty user name\n",
    "    df = process_who_column(messages_file)\n",
    "\n",
    "    # Preprocess messages: Add usernames and normalize UTF-8\n",
    "    df = preprocess_messages_with_usernames(df)\n",
    "\n",
    "    # Select the last 300 rows\n",
    "    df = df.iloc[0:150]\n",
    "\n",
    "    # Ensure DataFrame has Category and Subcategory columns\n",
    "    if \"Category\" not in df.columns:\n",
    "        df[\"Category\"] = \"\"\n",
    "    if \"Subcategory\" not in df.columns:\n",
    "        df[\"Subcategory\"] = \"\"\n",
    "\n",
    "    # Add Token Count column\n",
    "    df = add_token_count_column(df, column_name=\"Processed Content\", new_column_name=\"Token Count\")\n",
    "\n",
    "    # Load categories file\n",
    "    categories = load_categories_from_file(categories_file)\n",
    "    \n",
    "    # Initialize open ai client\n",
    "    client = openai\n",
    "\n",
    "    # Process messages in batches\n",
    "    df, categories = process_messages_in_batches(\n",
    "        client=client,\n",
    "        df=df,\n",
    "        categories_file=categories_file,\n",
    "        token_limit=1500, ## TODO: see if it's working for testing the auto handling max token bug\n",
    "        overlap_token_limit=400\n",
    "    )\n",
    "\n",
    "    # Save updated DataFrame to a CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nUpdated CSV saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c26eefd3-2aaa-4448-8956-cbb1c9714b92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 35\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m client \u001b[38;5;241m=\u001b[39m openai\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Process messages in batches\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m df, categories \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_messages_in_batches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategories_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m## TODO: see if it's working for testing the auto handling max token bug\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverlap_token_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\n\u001b[1;32m     41\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Save updated DataFrame to a CSV file\u001b[39;00m\n\u001b[1;32m     44\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(output_file, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[10], line 67\u001b[0m, in \u001b[0;36mprocess_messages_in_batches\u001b[0;34m(client, df, categories_file, token_limit, overlap_token_limit)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Build prompt and process batch\u001b[39;00m\n\u001b[1;32m     66\u001b[0m prompt \u001b[38;5;241m=\u001b[39m build_prompt(categories, batch_messages)\n\u001b[0;32m---> 67\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing rows \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_batch_start\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_batch_end\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with total tokens \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and overlap tokens \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moverlap_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw GPT Response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, response)  \u001b[38;5;66;03m# Print raw response for debugging\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mprocess_batch\u001b[0;34m(client, prompt)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mProcess a batch of messages using GPT.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/machine_learning/myenv/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/machine_learning/myenv/lib/python3.11/site-packages/openai/resources/chat/completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/machine_learning/myenv/lib/python3.11/site-packages/openai/_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1268\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1277\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1278\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1279\u001b[0m     )\n\u001b[0;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/machine_learning/myenv/lib/python3.11/site-packages/openai/_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/machine_learning/myenv/lib/python3.11/site-packages/openai/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1045\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/machine_learning/myenv/lib/python3.11/site-packages/openai/_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/machine_learning/myenv/lib/python3.11/site-packages/openai/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1045\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/machine_learning/myenv/lib/python3.11/site-packages/openai/_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/machine_learning/myenv/lib/python3.11/site-packages/openai/_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1060\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1065\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1070\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8728bd6e-edd6-4e82-8e4d-38ee62cd1610",
   "metadata": {},
   "source": [
    "## Catogorized Message Filtering again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0759ead0-93af-460d-9d0c-3ad89dab4cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "csv_file_path = './categorized_300_messages.csv'\n",
    "output_file = './refined_categorized_300_messages.csv'\n",
    "categories_file = \"./categories.json\"\n",
    "\n",
    "# Load the CSV into a pandas DataFrame\n",
    "df_categorized = pd.read_csv(csv_file_path)\n",
    "\n",
    "print(len(df_categorized[df_categorized['Category'] != 'Uncategorized']))\n",
    "\n",
    "df_categorized = df_categorized[df_categorized['Category'].str.strip().str.lower() != 'uncategorized']\n",
    "\n",
    "# Ensure the 'processed content' column exists\n",
    "if 'Processed Content' not in df_categorized.columns:\n",
    "    raise ValueError(\"The CSV does not contain a 'processed content' column.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11124fe0-021f-4533-9173-de384ab0bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai\n",
    "\n",
    "categories = load_categories_from_file(categories_file)\n",
    "\n",
    "# Process messages in batches\n",
    "df_refined_categorized, categories = process_messages_in_batches(\n",
    "    client=client,\n",
    "    df=df_categorized,\n",
    "    categories_file=categories_file,\n",
    "    token_limit=3000, ## TODO: see if it's working for testing the auto handling max token bug\n",
    "    overlap_token_limit=400\n",
    ")\n",
    "\n",
    "# Save updated DataFrame to a CSV file\n",
    "df_refined_categorized.to_csv(output_file, index=False)\n",
    "\n",
    "df_refined_categorized_only_with_catgory = df_refined_categorized[df_refined_categorized['Category'].str.strip().str.lower() != 'uncategorized']\n",
    "df_refined_categorized_only_with_catgory.to_csv('./df_refined_categorized_only_with_catgory.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7bebd5-c3c6-4e7b-b0ba-e317f7a84006",
   "metadata": {},
   "source": [
    "## Building Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec62f6fc-8b30-4635-906b-ab5b5e94455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_summary_in_batches(client, df, categories_file, batch_size=50):\n",
    "#     # Load categories\n",
    "#     categories = load_categories_from_file(categories_file)\n",
    "    \n",
    "#     # Filter out uncategorized and get the combined content list\n",
    "#     df_refined_categorized = df[df['Category'].str.strip().str.lower() != 'uncategorized'].copy()\n",
    "#     all_contents = df_refined_categorized['Processed Content'].astype(str).tolist()\n",
    "    \n",
    "#     # We'll store the summaries for each batch\n",
    "#     summaries = []\n",
    "\n",
    "#     def build_summary_prompt(categories, messages):\n",
    "#         prompt = \"\"\"\n",
    "        \n",
    "#         The following is a collection of messages and their associated categories. Please analyze the content and provide a detailed generalization.\n",
    "        \n",
    "#         Your task is to:\n",
    "#         1. Identify the main points and concerns expressed in the messages.\n",
    "#         2. Not all the messages are related to living problems; focus only on relevant ones.\n",
    "#         3. Provide a balanced overview of the categories and subcategories, ensuring no critical insights are missed, even if mentioned only once.\n",
    "#         4. Summarize the key topics or themes across the messages, focusing on their meaning and relevance rather than repetition.\n",
    "#         5. Group related issues into broader themes or perspectives to create a cohesive analysis.\n",
    "        \n",
    "#         Here are the categories for living problems:\n",
    "        \n",
    "#         \"\"\"\n",
    "        \n",
    "#         # Add categories and subcategories\n",
    "#         for category, subs in categories.items():\n",
    "#             prompt += f\"{category}\\n\"\n",
    "#             for sub in subs:\n",
    "#                 prompt += f\"    • {sub}\\n\"\n",
    "                \n",
    "#         prompt += \"\\nMessages:\\n\"\n",
    "        \n",
    "#         # Add numbers to messages for better separation\n",
    "#         for idx, message in enumerate(messages, 1):\n",
    "#             prompt += f\"{idx}. {message.strip()}\\n\"\n",
    "    \n",
    "#         return prompt\n",
    "    \n",
    "#     # Batch processing\n",
    "#     start_index = 0\n",
    "#     n = len(all_contents)\n",
    "    \n",
    "#     while start_index < n:\n",
    "#         end_index = start_index + batch_size\n",
    "        \n",
    "#         # Adjust the end index if it goes beyond the length of all_contents\n",
    "#         if end_index > n:\n",
    "#             end_index = n\n",
    "        \n",
    "#         # Get the current batch of messages\n",
    "#         current_batch = all_contents[start_index:end_index]\n",
    "        \n",
    "#         # Build a prompt with the current batch of messages\n",
    "#         # Also provide the original message indices for clarity if needed\n",
    "#         # The prompt builder can incorporate message indices if you adjust it accordingly.\n",
    "#         prompt = build_summary_prompt(categories, current_batch)\n",
    "        \n",
    "#         # Run the prompt\n",
    "#         response = process_batch(client, prompt)\n",
    "        \n",
    "#         # Store the response\n",
    "#         batch_summary = {\n",
    "#             \"batch_start_index\": start_index,\n",
    "#             \"batch_end_index\": end_index - 1,\n",
    "#             \"summary\": response\n",
    "#         }\n",
    "#         summaries.append(batch_summary)\n",
    "        \n",
    "#         # Calculate the next start index\n",
    "#         # We move forward by (batch_size - overlap) to create an overlapping window\n",
    "#         next_start = start_index + batch_size\n",
    "        \n",
    "#         # If next_start is not less than end_index, it means we've processed all messages\n",
    "#         if next_start >= n:\n",
    "#             break\n",
    "        \n",
    "#         start_index = next_start\n",
    "    \n",
    "#     return summaries\n",
    "\n",
    "# # Example usage:\n",
    "# # summaries = process_messages_in_batches(client, df_categorized, \"categories.txt\", batch_size=50, overlap=20)\n",
    "# # for summary in summaries:\n",
    "# #     print(\"From message index\", summary[\"batch_start_index\"], \"to\", summary[\"batch_end_index\"])\n",
    "# #     print(summary[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3817e484-29ae-4acd-8ab1-a4715debc6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: add tiktoken checking the token count to shrink if the input is too large\n",
    "\n",
    "def process_summary_in_batches(client, df, categories_file, initial_batch_size=50, min_batch_size=5):\n",
    "    # Load categories\n",
    "    categories = load_categories_from_file(categories_file)\n",
    "    \n",
    "    # Filter out uncategorized and get the combined content list\n",
    "    df_refined_categorized = df[df['Category'].str.strip().str.lower() != 'uncategorized'].copy()\n",
    "    all_contents = df_refined_categorized['Processed Content'].astype(str).tolist()\n",
    "    \n",
    "    # We'll store the summaries for each batch\n",
    "    summaries = []\n",
    "    skipped_indices = set()  # Track messages that are too large to process\n",
    "\n",
    "    def build_summary_prompt(categories, messages, global_start_idx):\n",
    "        prompt = \"\"\"\n",
    "        The following is a collection of messages and their associated categories. Please analyze the content and provide a detailed generalization.\n",
    "        \n",
    "        Your task is to:\n",
    "        1. Identify the main points and concerns expressed in the messages.\n",
    "        2. Not all the messages are related to living problems; focus only on relevant ones.\n",
    "        3. Provide a balanced overview of the categories and subcategories, ensuring no critical insights are missed, even if mentioned only once.\n",
    "        4. For each subcategory, highligh two to three specific points from the releant messages. (with user name and no message number).\n",
    "        5. Show how many user concent about the problem when mentioning the problem like \"4 users worried about...\", \"7 users mentioned...\" etc)\n",
    "        6. If what they mention is importnat, never skip that specific things they mentioned.\n",
    "        \n",
    "        Here are the categories for living problems:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Add categories and subcategories\n",
    "        for category, subs in categories.items():\n",
    "            prompt += f\"{category}\\n\"\n",
    "            for sub in subs:\n",
    "                prompt += f\"    • {sub}\\n\"\n",
    "                \n",
    "        prompt += \"\\nMessages:\\n\"\n",
    "        \n",
    "        # Add global numbering to messages\n",
    "        for idx, message in enumerate(messages, global_start_idx):\n",
    "            prompt += f\"{idx}. {message.strip()}\\n\"\n",
    "    \n",
    "        return prompt\n",
    "    \n",
    "    n = len(all_contents)\n",
    "    batch_size = initial_batch_size\n",
    "    start_index = 0\n",
    "\n",
    "    while start_index < n:\n",
    "        raw_end_index = min(start_index + batch_size, n)\n",
    "        # Exclude skipped messages\n",
    "        batch_indices = [i for i in range(start_index, raw_end_index) if i not in skipped_indices]\n",
    "\n",
    "        # If no valid messages to process\n",
    "        if not batch_indices:\n",
    "            # Move on and reset\n",
    "            start_index = raw_end_index\n",
    "            batch_size = initial_batch_size\n",
    "            continue\n",
    "\n",
    "        # Build the current batch\n",
    "        current_batch = [all_contents[i] for i in batch_indices]\n",
    "        global_start_idx = batch_indices[0] + 1  # Adjust for global numbering\n",
    "        prompt = build_summary_prompt(categories, current_batch, global_start_idx)\n",
    "        \n",
    "        try:\n",
    "            response = process_batch(client, prompt)\n",
    "        except TokenTooLargeError:\n",
    "            print(f\"Batch {start_index} to {raw_end_index} too large. Reducing batch size.\")\n",
    "            # Reduce the batch size if possible\n",
    "            if batch_size > min_batch_size:\n",
    "                batch_size = max(batch_size // 2, min_batch_size)\n",
    "            else:\n",
    "                # Already at min_batch_size and still can't process\n",
    "                print(f\"Cannot reduce batch size further for batch {start_index} to {raw_end_index}. Skipping these messages.\")\n",
    "                for i in batch_indices:\n",
    "                    summaries.append({\n",
    "                        \"batch_start_index\": i,\n",
    "                        \"batch_end_index\": i,\n",
    "                        \"summary\": \"SkippedTooLarge\"\n",
    "                    })\n",
    "                    skipped_indices.add(i)\n",
    "                start_index = raw_end_index\n",
    "            continue\n",
    "\n",
    "        if response == \"Error\":\n",
    "            print(f\"Error processing batch {start_index} to {raw_end_index}. Marking as 'ErrorOccurred'.\")\n",
    "            for i in batch_indices:\n",
    "                summaries.append({\n",
    "                    \"batch_start_index\": i,\n",
    "                    \"batch_end_index\": i,\n",
    "                    \"summary\": \"ErrorOccurred\"\n",
    "                })\n",
    "            start_index = raw_end_index\n",
    "            continue\n",
    "\n",
    "        # Successful response, store it\n",
    "        batch_summary = {\n",
    "            \"batch_start_index\": batch_indices[0],\n",
    "            \"batch_end_index\": batch_indices[-1],\n",
    "            \"summary\": response\n",
    "        }\n",
    "        summaries.append(batch_summary)\n",
    "\n",
    "        next_start = start_index + batch_size\n",
    "        \n",
    "        # If next_start is not beyond raw_end_index, move there, else jump to raw_end_index\n",
    "        if next_start <= raw_end_index:\n",
    "            start_index = next_start\n",
    "        else:\n",
    "            start_index = raw_end_index\n",
    "\n",
    "        # Reset batch size after successful processing\n",
    "        batch_size = initial_batch_size\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ddc41-2ed3-4509-a6c0-6ffd668ac5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = process_summary_in_batches(\n",
    "    client=client,\n",
    "    df=df_categorized,\n",
    "    categories_file=categories_file,\n",
    "    initial_batch_size=10,\n",
    "    min_batch_size=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7050933-3ddc-4db5-8a88-101420a1c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bc0801-2e82-4e6b-8ac4-a03e0554bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: add tiktoken checking the token count for every individual subsummary and to shrink it if it is too large\n",
    "## unless it's the last output\n",
    "\n",
    "def merge_summaries(client, sub_summaries, max_iterations=10):\n",
    "    \"\"\"\n",
    "    Hierarchically merges a list of sub-summaries into one comprehensive summary.\n",
    "\n",
    "    This function uses a round-by-round, pairwise merging approach (similar to a merge sort).\n",
    "    In each round, the list of current summaries is traversed and:\n",
    "      - Adjacent pairs of summaries are merged into a single summary.\n",
    "      - If there's an odd one out, it just carries over to the next round.\n",
    "    \n",
    "    The process continues until only one summary remains or we reach max_iterations.\n",
    "\n",
    "    Arguments:\n",
    "        client: The OpenAI client or similar model-processing object.\n",
    "        sub_summaries: A list of dicts, each containing keys:\n",
    "            - 'batch_start_index': The starting index of the batch of messages summarized.\n",
    "            - 'batch_end_index': The ending index of the batch of messages summarized.\n",
    "            - 'summary': The textual summary itself.\n",
    "        max_iterations: A safeguard to prevent infinite loops if something goes wrong.\n",
    "\n",
    "    Returns:\n",
    "        A single, final summary string after all merges are complete.\n",
    "    \"\"\"\n",
    "\n",
    "    def build_merge_prompt(summary_a, summary_b, index_range_a, index_range_b, final_merge):\n",
    "        \"\"\"\n",
    "        Builds the merge prompt step by step with conditional logic for Rule 7.\n",
    "    \n",
    "        Arguments:\n",
    "            summary_a: The first sub-summary.\n",
    "            summary_b: The second sub-summary.\n",
    "            index_range_a: Tuple representing the index range for summary_a.\n",
    "            index_range_b: Tuple representing the index range for summary_b.\n",
    "            anonymize: Boolean flag indicating whether to anonymize usernames.\n",
    "    \n",
    "        Returns:\n",
    "            A string containing the full merge prompt.\n",
    "        \"\"\"\n",
    "        # Initialize the base prompt\n",
    "        prompt = \"\"\"\n",
    "        Use the information provided in each sub-summary, including any indications of how many messages mention each theme, \n",
    "        and ensure that you include the specific examples cited in these sub-summaries.\n",
    "    \n",
    "        Please:\n",
    "        1. Merge two sub summaries into one summary as they describe the same data.\n",
    "        2. Combine frequency counts or message references (e.g., \"4 users worried about...\") from both sub-summaries.\n",
    "           If Sub-summary A says X messages and Sub-summary B says Y messages for a theme, \n",
    "           reflect the total combined frequency (X+Y, or a best estimate if exact counts are unclear).\n",
    "        3. **Do not omit specific examples mentioned in the sub-summaries.** For each subcategory, highlight two to three concrete points \n",
    "           based directly on the examples provided. Make sure these points are specific (e.g., mention hair salon quality issues, \n",
    "           difficulties finding lawyers, confusion over tax documentation).\n",
    "        4. Even if a topic is mentioned only once, ensure that it is included in the merged summary.\n",
    "        5. Emphasize the meaning and relevance of these themes, rather than simply repeating categories.\n",
    "        6. Do not refer to this as a \"merged summary\" in the final output. Just present the final \"Summary:\" with categories, frequencies, and specific examples.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Conditionally add Rule 7\n",
    "        if final_merge:\n",
    "            prompt += \"\"\"\n",
    "        7. **Hide the usernames for privacy** Replace specific names with references like 'one user,' 'some users,' or 'User X.'\n",
    "            \"\"\"\n",
    "    \n",
    "        # Append the sub-summaries\n",
    "        prompt += f\"\"\"\n",
    "        \n",
    "        Sub-summary A (Messages {index_range_a[0]} to {index_range_a[1]}):\n",
    "        {summary_a}\n",
    "        \n",
    "        Sub-summary B (Messages {index_range_b[0]} to {index_range_b[1]}):\n",
    "        {summary_b}\n",
    "        \"\"\"\n",
    "    \n",
    "        return prompt\n",
    "\n",
    "\n",
    "    iteration = 0\n",
    "    current_summaries = sub_summaries\n",
    "\n",
    "    # Keep merging until we have a single summary or we exceed max_iterations\n",
    "    while len(current_summaries) > 1 and iteration < max_iterations:\n",
    "        next_round = []\n",
    "\n",
    "        # Process in pairs (0&1, 2&3, ...) each iteration\n",
    "        for i in range(0, len(current_summaries), 2):\n",
    "            if i + 1 < len(current_summaries):\n",
    "                s1 = current_summaries[i]\n",
    "                s2 = current_summaries[i+1]\n",
    "\n",
    "                # Build and send prompt for merging this pair\n",
    "                prompt = build_merge_prompt(\n",
    "                    s1['summary'], \n",
    "                    s2['summary'],\n",
    "                    (s1['batch_start_index'], s1['batch_end_index']),\n",
    "                    (s2['batch_start_index'], s2['batch_end_index']),\n",
    "                    final_merge= (len(current_summaries) == 2)\n",
    "                )\n",
    "                \n",
    "                merged_response = process_batch(client, prompt)\n",
    "\n",
    "                # Create a new merged summary dict\n",
    "                new_summary = {\n",
    "                    \"batch_start_index\": s1['batch_start_index'],\n",
    "                    \"batch_end_index\": s2['batch_end_index'],\n",
    "                    \"summary\": merged_response\n",
    "                }\n",
    "                next_round.append(new_summary)\n",
    "            else:\n",
    "                # Odd summary out: just carry it forward unchanged\n",
    "                next_round.append(current_summaries[i])\n",
    "        \n",
    "        # Prepare for the next round of merging\n",
    "        current_summaries = next_round\n",
    "        iteration += 1\n",
    "\n",
    "    # If we're left with one summary, that's our final result\n",
    "    if len(current_summaries) == 1:\n",
    "        return current_summaries[0]['summary']\n",
    "    else:\n",
    "        raise RuntimeError(\"Merging did not finish properly within max_iterations.\")\n",
    "\n",
    "# Example usage:\n",
    "# final_summary = merge_summaries_hierarchically(client, summaries)\n",
    "# print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b313a-56e3-40c0-a934-b84355faaf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: solve the same max token bug in merge_summaries()\n",
    "\n",
    "final_summary = merge_summaries(\n",
    "    client=client,\n",
    "    sub_summaries = summaries,\n",
    "    max_iterations = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ca4023-14e8-4a49-8ae1-094a9128b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1805c4eb-64dc-42fa-82ee-b57859a291b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_summary_prompt(message):\n",
    "    prompt = \"\"\"\n",
    "    \n",
    "    “Summarize the following detailed report into 2-3 sentences, focusing only on the most critical themes without going into specific examples or details\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    prompt += message\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2759709-1940-454c-95ba-d24a9f4d90fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_process_summary(client, message, summary_type=\"simple\"):\n",
    "    \"\"\"\n",
    "    Generates a summary prompt and processes it based on the summary type.\n",
    "\n",
    "    Args:\n",
    "        client: The client object for processing the batch.\n",
    "        message: The detailed report to be summarized.\n",
    "        summary_type: The type of summary - \"simple\" for 2-3 sentences, \"middle\" for 5-7 sentences.\n",
    "\n",
    "    Returns:\n",
    "        The processed summary response.\n",
    "    \"\"\"\n",
    "    if summary_type == \"simple\":\n",
    "        prompt = \"\"\"\n",
    "        Summarize the following detailed report into 2 sentences, focusing only on the most critical themes. Highlight the major challenges and topics without going into secondary details or specifics.\n",
    "        \"\"\"\n",
    "    elif summary_type == \"middle\":\n",
    "        prompt = \"\"\"\n",
    "        Summarize the following detailed report into 5-7 sentences, capturing the main themes and challenges in a balanced way and mention how they say it in the original text if it's imprtant. Group related issues where possible, and provide a brief insight into the topics discussed without listing all details.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid summary_type. Choose 'simple' or 'middle'.\")\n",
    "\n",
    "    prompt += f\"\\n\\n{message}\"\n",
    "    response = process_batch(client, prompt)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ead0d-3911-4aa2-9447-606a94403304",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_type = \"simple\"  # Change to \"middle\" for a mid-level summary\n",
    "summary_response = build_and_process_summary(client, final_summary, summary_type)\n",
    "print(summary_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332284b2-a0a5-49c0-9cc9-6fd10f84cfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_type = \"middle\"  # Change to \"middle\" for a mid-level summary\n",
    "summary_response = build_and_process_summary(client, final_summary, summary_type)\n",
    "print(summary_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526780e-644e-4eae-b58e-3e4ad5e53a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "        Use the information provided in each sub-summary, including any indications of how many messages mention each theme, \n",
    "        and ensure that you include the specific examples cited in these sub-summaries.\n",
    "    \n",
    "        Please:\n",
    "        1. Merge two sub summary into one summary as it's decribing the same data.\n",
    "        2. Combine frequency counts or message references (e.g., \"4 users worried about...\") from both sub-summaries.\n",
    "           If Sub-summary A says X messages and Sub-summary B says Y messages for a theme, \n",
    "           reflect the total combined frequency (X+Y, or a best estimate if exact counts are unclear).\n",
    "        3. **Do not omit specific examples mentioned in the sub-summaries.** For each subcategory, highlight two to three concrete points \n",
    "           based directly on the examples provided. Make sure these points are specific (e.g., mention hair salon quality issues, \n",
    "           difficulties finding lawyers, confusion over tax documentation).\n",
    "        4. Even if a topic is mentioned only once, ensure that it is included in the merged summary.\n",
    "        5. Emphasize the meaning and relevance of these themes, rather than simply repeating categories.\n",
    "        6. Do not refer to this as a \"merged summary\" in the final output. Just present the final \"Summary:\" with categories, frequencies, and specific examples.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
