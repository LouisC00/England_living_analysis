{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ac26ffe-65c7-43e8-9a43-2ca73fefbed1",
   "metadata": {},
   "source": [
    "### **Step 1: Example Conversation in Cantonese (Discussing Living Issues)**\n",
    "```\n",
    "UserA: 最近租金又加咗，你嗰邊情況點呀？\n",
    "UserB: 真係頂唔順，我而家諗緊要唔要搬返去屋企同父母住。\n",
    "UserA: 我都明白，你覺得返去屋企會唔會舒服啲？\n",
    "UserB: 未必啦，雖然慳到租金，但係始終冇自己嘅私人空間。\n",
    "UserA: 係呀，呢個係一個問題。我諗住同室友傾下，睇下有冇可能一齊搬去平啲嘅地方。\n",
    "UserC: 係喇，咁樣可能會好啲。你哋有冇諗住去邊區？\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d5d4e-a7a9-4096-a542-86558a57dc70",
   "metadata": {},
   "source": [
    "### **Step 2: Telegram Export Example in JSON Format**\n",
    "If exported from Telegram, the `.json` file may look like this:\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"id\": 1,\n",
    "    \"date\": \"2024-09-28T10:00:00\",\n",
    "    \"from\": \"UserA\",\n",
    "    \"text\": \"最近租金又加咗，你嗰邊情況點呀？\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 2,\n",
    "    \"date\": \"2024-09-28T10:05:00\",\n",
    "    \"from\": \"UserB\",\n",
    "    \"text\": \"真係頂唔順，我而家諗緊要唔要搬返去屋企同父母住。\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 3,\n",
    "    \"date\": \"2024-09-28T10:10:00\",\n",
    "    \"from\": \"UserA\",\n",
    "    \"text\": \"我都明白，你覺得返去屋企會唔會舒服啲？\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 4,\n",
    "    \"date\": \"2024-09-28T10:15:00\",\n",
    "    \"from\": \"UserB\",\n",
    "    \"text\": \"未必啦，雖然慳到租金，但係始終冇自己嘅私人空間。\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 5,\n",
    "    \"date\": \"2024-09-28T10:20:00\",\n",
    "    \"from\": \"UserA\",\n",
    "    \"text\": \"係呀，呢個係一個問題。我諗住同室友傾下，睇下有冇可能一齊搬去平啲嘅地方。\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 6,\n",
    "    \"date\": \"2024-09-28T10:25:00\",\n",
    "    \"from\": \"UserC\",\n",
    "    \"text\": \"係喇，咁樣可能會好啲。你哋有冇諗住去邊區？\"\n",
    "  }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97755e53-98a2-4f33-8524-9ce8d3bcb4c1",
   "metadata": {},
   "source": [
    "### **Step 3: Input into the GPT-4 API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed383ed-1d6c-45f8-9061-bfde7ce9fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "# Initialize OpenAI API key\n",
    "openai.api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "# Load the conversation JSON file\n",
    "with open('telegram_conversation.json', 'r', encoding='utf-8') as file:\n",
    "    conversation_data = json.load(file)\n",
    "\n",
    "# Prepare the conversation text for input to the API\n",
    "conversation_text = \"\"\n",
    "for message in conversation_data:\n",
    "    conversation_text += f\"{message['from']}: {message['text']}\\n\"\n",
    "\n",
    "# Use the OpenAI API to analyze and label the conversation\n",
    "def analyze_conversation(conversation_text):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant that analyzes and labels conversations about living issues.\"},\n",
    "            {\"role\": \"user\", \"content\": conversation_text}\n",
    "        ]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Get analysis result\n",
    "analysis_result = analyze_conversation(conversation_text)\n",
    "print(analysis_result)\n",
    "\n",
    "# Save the result to a text file for further analysis\n",
    "with open('analysis_result.txt', 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(analysis_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6661bbdd-8ed7-4dfa-8685-b51e947448a9",
   "metadata": {},
   "source": [
    "### **Step 4a: Example Output from GPT-4 Analysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffbdace-336a-4e1f-b544-8adf685f9791",
   "metadata": {},
   "source": [
    "**Labels:**\n",
    "- Main Topic: Housing and Rent\n",
    "- Subtopics:\n",
    "  - Rising Rent Costs\n",
    "  - Living with Parents\n",
    "  - Privacy Concerns\n",
    "  - Roommate Arrangements\n",
    "  - Financial Planning\n",
    "\n",
    "**Sentiment Analysis:**\n",
    "- User_123: Concerned but proactive\n",
    "- User_456: Frustrated and uncertain\n",
    "- User_789: Neutral but interested\n",
    "\n",
    "**Engagement Analysis:**\n",
    "- Unique Users Discussing Main Topic: 50\n",
    "- Total Mentions of Main Topic: 150\n",
    "- Repeat Count (Total Mentions/Unique Users): 3.0\n",
    "- Time Span in a month: 2 weeks\n",
    "- Total Word Count by User_123: 150 words\n",
    "- Total Word Count by User_456: 120 words\n",
    "- Total Word Count by User_789: 85 words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb395861-78c4-48eb-9acb-7dadb6ac6ae6",
   "metadata": {},
   "source": [
    "### **Step 4b: Example JSON Ouput from GPT-4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf09c7-72ca-4f2d-a3fa-47e01b683164",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"main_topic\": \"Housing and Rent\",\n",
    "    \"subtopics\": [\n",
    "        \"Rising Rent Costs\",\n",
    "        \"Living with Parents\",\n",
    "        \"Privacy Concerns\",\n",
    "        \"Roommate Arrangements\",\n",
    "        \"Financial Planning\"\n",
    "    ],\n",
    "    \"sentiment_analysis\": [\n",
    "        {\n",
    "            \"user_id\": \"UserA\",\n",
    "            \"sentiment\": \"Concerned but proactive\",\n",
    "            \"total_word_count\": 45\n",
    "        },\n",
    "        {\n",
    "            \"user_id\": \"UserB\",\n",
    "            \"sentiment\": \"Frustrated and uncertain\",\n",
    "            \"total_word_count\": 40\n",
    "        }\n",
    "    ],\n",
    "    \"engagement_analysis\": {\n",
    "        \"unique_users_discussing\": 2,\n",
    "        \"total_mentions_of_topic\": 6,\n",
    "        \"repeat_count\": 3.0,\n",
    "        \"time_span\": \"25 minutes\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ec0663-bfb3-457a-b92a-27f1c2efe8ab",
   "metadata": {},
   "source": [
    "### **Step 5: Extract data from GPT JSON output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2831d-86b3-4c24-9d8b-16bf43535fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_topic = re.search(r\"Main Topic:\\s*(.*)\", gpt_output).group(1)\n",
    "subtopics = re.findall(r\"- (.*)\", gpt_output.split(\"Subtopics:\")[1].split(\"**\")[0].strip())\n",
    "\n",
    "sentiment_section = gpt_output.split(\"**Sentiment Analysis:**\")[1].split(\"**Engagement Analysis:**\")[0]\n",
    "user_sentiments = re.findall(r\"- (User_\\d+): (.*)\", sentiment_section.strip())\n",
    "\n",
    "engagement_section = gpt_output.split(\"**Engagement Analysis:**\")[1]\n",
    "unique_users = int(re.search(r\"Unique Users Discussing Main Topic:\\s*(\\d+)\", engagement_section).group(1))\n",
    "total_mentions = int(re.search(r\"Total Mentions of Main Topic:\\s*(\\d+)\", engagement_section).group(1))\n",
    "repeat_count = float(re.search(r\"Repeat Count.*:\\s*([\\d.]+)\", engagement_section).group(1))\n",
    "time_span = re.search(r\"Time Span:\\s*(.*)\", engagement_section).group(1)\n",
    "\n",
    "user_word_counts = re.findall(r\"Total Word Count by (User_\\d+):\\s*(\\d+) words\", engagement_section)\n",
    "\n",
    "# Create a DataFrame to capture the data\n",
    "user_data = []\n",
    "for user_id, sentiment in user_sentiments:\n",
    "    word_count = next((int(count) for uid, count in user_word_counts if uid == user_id), None)\n",
    "    user_data.append({\n",
    "        'User_ID': user_id,\n",
    "        'Sentiment': sentiment,\n",
    "        'Word_Count': word_count\n",
    "    })\n",
    "\n",
    "user_df = pd.DataFrame(user_data)\n",
    "\n",
    "# Displaying the main analysis DataFrame\n",
    "main_analysis_df = pd.DataFrame({\n",
    "    'Main Topic': [main_topic],\n",
    "    'Subtopics': [', '.join(subtopics)],\n",
    "    'Unique Users Discussing': [unique_users],\n",
    "    'Total Mentions of Topic': [total_mentions],\n",
    "    'Repeat Count': [repeat_count],\n",
    "    'Time Span': [time_span],\n",
    "})\n",
    "\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Main Analysis\", dataframe=main_analysis_df)\n",
    "tools.display_dataframe_to_user(name=\"User Sentiments and Word Counts\", dataframe=user_df)\n",
    "\n",
    "# Showing counts of unique user engagement and word averages\n",
    "print(\"\\nTotal Unique Users Discussing the Topic:\", unique_users)\n",
    "print(\"\\nRepeat Count (Mentions/User):\", repeat_count)\n",
    "print(\"\\nUser Engagement Summary:\")\n",
    "print(user_df.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
